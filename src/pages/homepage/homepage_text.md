
# Empower businesses and individuals to build and maintain their own reward models. 

## Reward models enable reinforcement learning with human feedback in Large Language Models. 

However, constructing a reward model is challenging. The two major classes of problems are: 
- Coordinating preference data over a sufficiently large human group, and resolving preference ambiguities. 
- Using the reward model to train the LLM is a sensitive process and prone to reward-hacking and other perturbations. 

There are advantages to reward models that outweigh the challenges:
- You own your reward model, it is your IP 
- Reward models are portable, you can reuse the same reward model even if you switch LLM backends
- Reward models are an order of magnitude smaller in size, and thus can be stored and run from your phone

## We beleive the future of AI is in decentralized reward models. 

## Our mission is to build tools that enable RLHF at scale. 

## Our vision is to align Artificial Intelligence to humans. 
